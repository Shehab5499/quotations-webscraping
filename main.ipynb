{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from time import sleep\n",
    "from random import randint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dowloading Main HTML pages\n",
    "! No need to run unless missing files issue exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a folder to store html pages containing the quotes\n",
    "folder_name='html_pages'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.mkdir(folder_name)\n",
    "if not os.path.exists(folder_name+'/main_pgs'):\n",
    "    os.mkdir(folder_name+'/main_pgs')\n",
    "# Downloading html pages containing the quotes\n",
    "'''\n",
    "The website:\n",
    "https://www.passiton.com/inspirational-quotes\n",
    "consists of 77 pages -> viewed on 26/12/2022.\n",
    "Each page consists of approximately 33 link/linked-image to a subpage\n",
    "Within each subpage lies a quote, category, author name\n",
    "'''\n",
    "URL = 'https://www.passiton.com/inspirational-quotes?page='\n",
    "\n",
    "i = 0\n",
    "num_pages = 77\n",
    "''' The first loop iterates over pages to parse for links to subpages'''\n",
    "for pg_no in range(1, num_pages+1):\n",
    "    newfile = os.path.joint(folder_name+'/main_pgs',f'pg{pg_no}.html')\n",
    "    if not os.path.exists(newfile):\n",
    "        print(f'{URL}{pg_no}')\n",
    "        response = requests.get(f'{URL}{pg_no}')\n",
    "        #  sleep(randint(2,5)) \n",
    "        # To avoid blacklisting our ip address due to numerous server requests in very short time\n",
    "        soup = bs(response.content, 'html5lib')\n",
    "        with open(folder_name+'/main_pgs'+f'/pg{pg_no}.html','wb') as main_page:\n",
    "            main_page.write(response.content)\n",
    "    print(f'{URL}{pg_no}')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading HTML Subpages - \n",
    "! No need to run unless missing files issue exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://www.passiton.com/'\n",
    "folder_name='html_pages'\n",
    "\n",
    "''' The second loop iterates over links download subpages'''\n",
    "for pg_no, main_pg in enumerate(os.listdir('html_pages\\main_pgs')):\n",
    "    f = os.path.join('html_pages/main_pgs',main_pg)\n",
    "    with open(f, encoding='utf-8') as html_file:\n",
    "        soup = bs(html_file, 'html5lib')\n",
    "# soup now contains a main page html file content\n",
    "    containers = soup.find_all('h5', class_='value_on_red')\n",
    "    for sub_no, con in enumerate(containers):\n",
    "        newfile = os.path.join(folder_name,f'pg{pg_no+1}-sub{sub_no+1}.html')\n",
    "        if not os.path.exists(newfile):\n",
    "            sub_url = con.find('a').get('href')\n",
    "            response = requests.get(f'{url}{sub_url}')\n",
    "            # sleep(1) \n",
    "            # To avoid blacklisting our ip address due to numerous server requests in very short time\n",
    "            print(f'{url}{sub_url}')\n",
    "            with open(folder_name+f'/pg{pg_no+1}-sub{sub_no+1}.html', 'wb') as html_file:\n",
    "                html_file.write(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'html_pages'\n",
    "df_list = []\n",
    "\n",
    "for html_file in os.listdir(folder_name):\n",
    "    if os.path.isfile(folder_name+'/'+html_file):\n",
    "        f = os.path.join(folder_name,html_file)\n",
    "        with open(f, encoding='utf-8') as html_doc:\n",
    "            soup = bs(html_doc, 'html5lib')\n",
    "            df_list.append(\n",
    "                {\n",
    "                'Quote': soup.find('h5').text[1:-1],\n",
    "                'Category': soup.find('h5',class_='value_on_red').text,\n",
    "                'Author': soup.find('p').text\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# xlsxwriter enine is used due to the existence of Unicode characters\n",
    "df = pd.DataFrame(df_list, columns=['Quote','Category','Author'])\n",
    "df.to_csv('quotes.csv', encoding='UTF-8',index=False)\n",
    "# Encoding: UTF-8 - is the most popular encoding of web pages\n",
    "print('DONE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e86967ef3b050da18bbe5ea151b663afcb6cafee6889da31b6e349bd88c989ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
